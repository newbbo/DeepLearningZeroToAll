{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newbbo/DeepLearningZeroToAll/blob/master/mldl_work/sentence_data_db_extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qxrqqigmPEk"
      },
      "outputs": [],
      "source": [
        "!pip install pymysql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dbMql-q2mPEl"
      },
      "outputs": [],
      "source": [
        "import pymysql\n",
        "import time\n",
        "\n",
        "def make_conn():\n",
        "    host = HOST\n",
        "    port = 3306\n",
        "    database = DBNAME\n",
        "    username = USERNAME\n",
        "    password = PASSWORD\n",
        "\n",
        "    conn = pymysql.connect(host= host, user= username, password=password, db=database, port=int(port),\n",
        "                        cursorclass=pymysql.cursors.DictCursor)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    return cursor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RgdiutWtmPEm"
      },
      "outputs": [],
      "source": [
        "# my_tag = 612\n",
        "\n",
        "def get_test_sent_list():\n",
        "    q_gram_sent_id = '''select cast(grammar_tbl.id as char) as gram_id, \n",
        "                        grammar_tbl.name as gram_name, \n",
        "                        cast(sent_gram.sentence_id as char) as sent_ids\n",
        "                        from grammar_tbl\n",
        "                        inner join sentence_grammar_relation_tbl as sent_gram on sent_gram.grammar_id = grammar_tbl.id\n",
        "                        inner join sentence_tbl as sent on sent.id = sent_gram.sentence_id\n",
        "                        where \n",
        "                        grammar_tbl.id = %s\n",
        "                        and sent.deleted_at is null\n",
        "                        and sent.structure is not null\n",
        "                        '''\n",
        "\n",
        "    cursor.execute(q_gram_sent_id, [my_tag])\n",
        "    gram_sent_id= cursor.fetchall()\n",
        "    gram_sent_ids = set()\n",
        "    for i in range (0, len(gram_sent_id)):\n",
        "        sent_ids = gram_sent_id[i]['sent_ids']\n",
        "        gram_sent_ids.add(sent_ids)\n",
        "    gram_sent_ids = list(gram_sent_ids)\n",
        "    return gram_sent_ids[0:2]\n",
        "\n",
        "\n",
        "def get_pieces_from_tag(gram_sent_ids):\n",
        "    kor_piece = []\n",
        "    eng_piece = []\n",
        "    gram_tags = []\n",
        "    label_pieces = []\n",
        "    pos_pieces = []\n",
        "    kidx_pieces = [] \n",
        "    htkidx_pieces = []\n",
        "    # print(gram_sent_ids)\n",
        "    \n",
        "    for i in range (0, len(gram_sent_ids)):\n",
        "        sentence_id = gram_sent_ids[i]\n",
        "        my_gram_id = str(my_tag)\n",
        "\n",
        "        korean_piece, english_piece, gram_tag, label_piece, pos_piece = gram_tag_by_chunck(sentence_id, my_gram_id)\n",
        "\n",
        "        for k in range (0, len(korean_piece)):\n",
        "            if len(gram_tag[k]) == 0: \n",
        "                pass\n",
        "            elif list(gram_tag[k])[0]!= my_gram_id:\n",
        "                pass\n",
        "            else:\n",
        "                kor_piece.append(korean_piece[k])\n",
        "                eng_piece.append(english_piece[k])\n",
        "                this_gram_tag = re.sub('[^0-9]', '', str(gram_tag[k]))\n",
        "                gram_tags.append(this_gram_tag)\n",
        "                label_pieces.append(label_piece[k])\n",
        "                pos_pieces.append(pos_piece[k])\n",
        "                kidx_pieces.append(tkidx_piece[k])\n",
        "                htkidx_pieces.append(htkidx_piece[k])\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            print('---- {} 번째 완료 / 총 {} 개')\n",
        "\n",
        "    chunk_gram_file = pd.DataFrame({'eng_piece' : [], 'kor_piece': [] , 'gram_tags' : []}) # , 'gram_new_label' : []})\n",
        "    chunk_gram_file['kor_piece'] = kor_piece\n",
        "    chunk_gram_file['eng_piece'] = eng_piece\n",
        "    chunk_gram_file['gram_tags'] = gram_tags\n",
        "    chunk_gram_file['label_pieces'] = label_pieces\n",
        "    chunk_gram_file['pos_pieces'] = pos_pieces\n",
        "    chunk_gram_file['kidx_pieces'] = kidx_pieces\n",
        "    chunk_gram_file['htkidx_pieces'] = htkidx_pieces\n",
        "\n",
        "\n",
        "    chunk_gram_file.to_csv('test.csv')\n",
        "    print('----- 파일 생성 완료 -----')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9Rfc70BgmPEn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "cursor = make_conn()\n",
        "\n",
        "# sentence_id = '147384335917'\n",
        "\n",
        "sent_gram_slash_info = dict()\n",
        "# sent_gram_slash_info['sentence_id'] = sentence_id\n",
        "\n",
        "##################################################################################################\n",
        "def get_sentence_word_count(sentence_id):\n",
        "    q_sentence_word_count = '''\n",
        "                        select JSON_LENGTH(sent.structure, '$.words') as word_cnt\n",
        "                        from sentence_tbl as sent\n",
        "                        where sent.id= %s'''\n",
        "    cursor.execute(q_sentence_word_count, [sentence_id])\n",
        "    word_cnt = cursor.fetchall()\n",
        "    word_cnt = word_cnt[0]['word_cnt']\n",
        "    if word_cnt == None:\n",
        "        word_cnt = 0\n",
        "    return word_cnt\n",
        "\n",
        "def get_sentence_slash_count(sentence_id):\n",
        "    q_sentence_slash_count = '''\n",
        "                        select JSON_LENGTH(sent.structure, '$.slashList') as slash_cnt\n",
        "                        from sentence_tbl as sent\n",
        "                        where sent.id= %s'''\n",
        "    cursor.execute(q_sentence_slash_count, [sentence_id])\n",
        "    slash_cnt = cursor.fetchall()\n",
        "    slash_cnt = slash_cnt[0]['slash_cnt']\n",
        "    if slash_cnt == None:\n",
        "        slash_cnt = 0 \n",
        "    return slash_cnt\n",
        "##################################################################################################\n",
        "\n",
        "# i 번째 단어에 대한 뜻과 문법태그 가져오기 \n",
        "def get_grammarid_for_each_word(word_cnt, sentence_id): \n",
        "    q_word_info = '''\n",
        "                        select JSON_EXTRACT(sent.structure, '$.words[%s].word') as word, \n",
        "                        JSON_EXTRACT(sent.structure, '$.words[%s].items[*].meanings') as meanings,\n",
        "                        JSON_EXTRACT(sent.structure, '$.words[%s].grammar[*].grammarId') as gram_id,\n",
        "                        JSON_EXTRACT(sent.structure, '$.words[%s].label') as label,\n",
        "                        JSON_EXTRACT(sent.structure, '$.words[%s].partOfSpeech') as pos,\n",
        "                        JSON_EXTRACT(sent.structure, '$.words[%s].tokenIndex') as tokenIndex,\n",
        "                        JSON_EXTRACT(sent.structure, '$.words[%s].headTokenIndex') as headTokenIndex\n",
        "                        from sentence_tbl as sent\n",
        "                        where sent.id= %s\n",
        "                        '''\n",
        "    \n",
        "    all_word_set = []\n",
        "    \n",
        "    for i in range(0, int(word_cnt)):\n",
        "        cursor.execute(q_word_info, [i, i, i, i, i, i, i, sentence_id])\n",
        "        word_info = cursor.fetchall()\n",
        "        word_set = {'word': 'none' , 'meanings' : 'none', 'gram_id' : 'none', 'label' : 'none', 'pos' : 'none',\n",
        "                   'tokenIndex' : 'none', 'headTokenIndex' : 'none'}\n",
        "        # word_set = dict()\n",
        "        word = word_info[0]['word']\n",
        "        word_meanings = word_info[0]['meanings']\n",
        "        word_grammarid = word_info[0]['gram_id']\n",
        "        word_label = word_info[0]['label']\n",
        "        word_pos = word_info[0]['pos']\n",
        "        word_tokenIndex = word_info[0]['tokenIndex']\n",
        "        word_headtokenIndex = word_info[0]['headTokenIndex']\n",
        "        \n",
        "        \n",
        "        word_set['word'] = word\n",
        "        word_set['meanings'] = word_meanings\n",
        "        word_set['gram_id'] = word_grammarid\n",
        "        word_set['label'] = word_label\n",
        "        word_set['pos'] = word_pos\n",
        "        word_set['tokenIndex'] = word_tokenIndex\n",
        "        word_set['headTokenIndex'] = word_headtokenIndex\n",
        "\n",
        "        all_word_set.append(word_set)\n",
        "    # print(all_word_set)\n",
        "    return all_word_set\n",
        "\n",
        "def get_slash_info(slash_cnt, sentence_id):\n",
        "    q_slash = '''\n",
        "                        select JSON_EXTRACT(sent.structure, '$.slashList[%s].start') as start,\n",
        "                        JSON_EXTRACT(sent.structure, '$.slashList[%s].end') as end,\n",
        "                        JSON_EXTRACT(sent.structure, '$.slashList[%s].translation') as translation\n",
        "                        from sentence_tbl as sent\n",
        "                        where sent.id= %s'''\n",
        "    slashes_info = []\n",
        "\n",
        "    for i in range (0, int(slash_cnt)):\n",
        "        each_slash = {'start': 'none' , 'end' : 'none', 'translation' : 'none'}\n",
        "        cursor.execute(q_slash, [i, i, i, sentence_id])\n",
        "        slash_info = cursor.fetchall()\n",
        "\n",
        "        each_slash['start'] = slash_info[0]['start']\n",
        "        each_slash['end'] = slash_info[0]['end']\n",
        "        each_slash['translation'] = slash_info[0]['translation']\n",
        "        slashes_info.append(each_slash)\n",
        "\n",
        "    # print(slashes_info)\n",
        "    return slashes_info\n",
        "\n",
        "# 문장에 들어있는 전체 문법 태그 가져오기 \n",
        "def get_grammarid_for_each_sentence(word_cnt, sentence_id):\n",
        "    q_grammar_id = '''\n",
        "                        select JSON_EXTRACT(sent.structure, '$.words[%s].grammar[0].grammarId') as gram_id\n",
        "                        from sentence_tbl as sent\n",
        "                        where sent.id= %s '''\n",
        "    gram_id_set = []\n",
        "\n",
        "    for i in range(0, int(word_cnt)):\n",
        "        cursor.execute(q_grammar_id, [i, sentence_id])\n",
        "        gram_id= cursor.fetchall()\n",
        "        gram_id = gram_id[0]['gram_id']\n",
        "        if gram_id != None:\n",
        "            gram_id_set.append(gram_id)\n",
        "    print(gram_id_set)\n",
        "    return  gram_id_set\n",
        "##################################################################################################\n",
        "\n",
        "def get_slash_all_info(word_cnt, slash_cnt, sentence_id):\n",
        "    word_info = get_grammarid_for_each_word(word_cnt, sentence_id)\n",
        "    slash_info = get_slash_info(slash_cnt, sentence_id)\n",
        "\n",
        "    for i in range (0, len(slash_info)):\n",
        "        start_idx = int(slash_info[i]['start'])\n",
        "        end_idx = int(slash_info[i]['end'])\n",
        "        \n",
        "        in_slash_list = []\n",
        "\n",
        "        for j in range(start_idx, end_idx+1):\n",
        "            in_slash_dict = {'word': 'none' , 'meanings' : 'none', 'gram_id' : 'none'}\n",
        "            in_slash_dict['word'] = word_info[j]['word']\n",
        "            in_slash_dict['meanings'] = word_info[j]['meanings']\n",
        "            in_slash_dict['gram_id'] = word_info[j]['gram_id']\n",
        "            in_slash_dict['label'] = word_info[j]['label']\n",
        "            in_slash_dict['pos'] = word_info[j]['pos']\n",
        "            in_slash_dict['tokenIndex'] = word_info[j]['tokenIndex']\n",
        "            in_slash_dict['headTokenIndex'] = word_info[j]['headTokenIndex']\n",
        "            \n",
        "            in_slash_list.append(in_slash_dict)\n",
        "        slash_info[i]['in_slash'] = in_slash_list\n",
        "    # print(slash_info)\n",
        "    return slash_info\n",
        "\n",
        "##################################################################################################\n",
        "\n",
        "def upgrade_meaning(sentence_id):\n",
        "    word_cnt = get_sentence_word_count(sentence_id)\n",
        "    slash_cnt = get_sentence_slash_count(sentence_id)\n",
        "\n",
        "    ans_string = ''\n",
        "    slash_info = get_slash_all_info(word_cnt, slash_cnt)\n",
        "    for i in range (0, len(slash_info)):\n",
        "        ans_string += slash_info[i]['translation']\n",
        "        for k in range (0, len(slash_info[i]['in_slash'])):\n",
        "            tmp = str(slash_info[i]['in_slash'][k]['meanings'])\n",
        "            ans_string += re.sub('[^가-힣]', '', tmp)\n",
        "    # print(ans_string)\n",
        "    return ans_string \n",
        "\n",
        "\n",
        "#################################################################################################\n",
        "# 문장 아이디, 영어 청크, 한글 청크, 문법 태그 생성\n",
        "\n",
        "def gram_tag_by_chunck(sentence_id, my_gram_id):\n",
        "    word_cnt = get_sentence_word_count(sentence_id)\n",
        "    slash_cnt = get_sentence_slash_count(sentence_id)\n",
        "\n",
        "    if word_cnt == 0 or slash_cnt == 0:\n",
        "        return [], [], []\n",
        "    else:\n",
        "        slash_info = get_slash_all_info(word_cnt, slash_cnt, sentence_id)\n",
        "        english_piece  = []\n",
        "        korean_piece = []\n",
        "        gram_tag = []\n",
        "        label_piece = []\n",
        "        pos_piece = []\n",
        "        tkidx_piece = []\n",
        "        htkidx_piece = []\n",
        "\n",
        "        english_ans = ''\n",
        "        label_ans = ''\n",
        "        pos_ans = ''\n",
        "        tkidx_ans = ''\n",
        "        htkid_ans = ''\n",
        "        my_gram_id = my_gram_id\n",
        "\n",
        "        for i in range (0, len(slash_info)):\n",
        "            korean_ans = str(slash_info[i]['translation'])\n",
        "            korean_ans = re.sub('[^a-zA-Z0-9가-힇 ]', '', korean_ans)\n",
        "            gram_tmp = []\n",
        "            for k in range (0, len(slash_info[i]['in_slash'])):\n",
        "                english_tmp = str(slash_info[i]['in_slash'][k]['word'])\n",
        "                english_tmp = re.sub('[^a-zA-Z0-9 ]', '', english_tmp)\n",
        "                \n",
        "                gram_id_tmp = slash_info[i]['in_slash'][k]['gram_id']\n",
        "                english_ans += english_tmp\n",
        "                english_ans += ' '\n",
        "                if gram_id_tmp is None:\n",
        "                    pass\n",
        "                elif my_gram_id in gram_id_tmp:\n",
        "                    gram_tmp.append(my_gram_id)\n",
        "                label_tmp = str(slash_info[i]['in_slash'][k]['label'])\n",
        "                label_ans += label_tmp\n",
        "                label_ans += ' '\n",
        "                pos_tmp = str(slash_info[i]['in_slash'][k]['pos'])\n",
        "                pos_ans += pos_tmp\n",
        "                pos_ans += ' '\n",
        "                tkidx_tmp = str(slash_info[i]['in_slash'][k]['tokenIndex'])\n",
        "                tkidx_ans += tkidx_tmp\n",
        "                tkidx_ans += ' '\n",
        "                htkidx_tmp = str(slash_info[i]['in_slash'][k]['headtokenIndex'])\n",
        "                htkid_ans += htkidx_tmp\n",
        "                htkid_ans += ' '\n",
        "                       \n",
        "            gram_tag.append(set(gram_tmp))\n",
        "            korean_piece.append(korean_ans)\n",
        "            english_piece.append(english_ans)\n",
        "            label_piece.append(label_ans)\n",
        "            pos_piece.append(pos_ans)\n",
        "            tkidx_piece.append(tkidx_ans)\n",
        "            htkidx_piece.append(htkid_ans)\n",
        "            english_ans = ''\n",
        "            label_ans = ''\n",
        "            pos_ans = ''\n",
        "            tkidx_ans = ''\n",
        "            htkid_ans = ''\n",
        "            \n",
        "\n",
        "        return korean_piece, english_piece, gram_tag, label_piece, pos_piece , tkidx_piece, htkidx_piece\n",
        "    \n",
        "    \n",
        "######\n",
        "#################################################################################################\n",
        "# slash_info의 정보를 열 별로 저장 \n",
        "\n",
        "def chunck_to_dataframe(slash_info):\n",
        "    if word_cnt == 0 or slash_cnt == 0:\n",
        "        return [], [], []\n",
        "    else:\n",
        "        english_piece  = []\n",
        "        korean_piece = []\n",
        "        gram_tag = []\n",
        "        label_piece = []\n",
        "        pos_piece = []\n",
        "        pos_label_piece = []\n",
        "        tkidx_piece = []\n",
        "        htkidx_piece = []\n",
        "\n",
        "        english_ans = ''\n",
        "        label_ans = ''\n",
        "        pos_ans = ''\n",
        "        pos_label_ans = ''\n",
        "        tkidx_ans = ''\n",
        "        htkid_ans = ''\n",
        "        my_gram_id = ''\n",
        "\n",
        "        for i in range (0, len(slash_info)):\n",
        "            korean_ans = str(slash_info[i]['translation'])\n",
        "            korean_ans = re.sub('[^a-zA-Z0-9가-힇 ]', '', korean_ans)\n",
        "            gram_tmp = ''\n",
        "            for k in range (0, len(slash_info[i]['in_slash'])):\n",
        "                english_tmp = str(slash_info[i]['in_slash'][k]['word'])\n",
        "                english_tmp = re.sub('[^a-zA-Z0-9 ]', '', english_tmp)\n",
        "                english_ans += english_tmp\n",
        "                \n",
        "                gram_id_tmp = slash_info[i]['in_slash'][k]['gram_id']\n",
        "                if gram_id_tmp is None:\n",
        "                    pass\n",
        "                else:\n",
        "                    gram_tmp += gram_id_tmp\n",
        "                                \n",
        "                # gram_tmp.append(gram_id_tmp)\n",
        "                label_tmp = re.sub('[^a-zA-Z0-9 ]', '', str(slash_info[i]['in_slash'][k]['label']))\n",
        "                label_ans += label_tmp\n",
        "                \n",
        "            \n",
        "                pos_tmp = re.sub('[^a-zA-Z0-9 ]', '', str(slash_info[i]['in_slash'][k]['pos']))\n",
        "                pos_ans += pos_tmp\n",
        "                \n",
        "                pos_label_ans += pos_tmp + ' '\n",
        "                pos_label_ans += label_tmp + ' '\n",
        "                \n",
        "                tkidx_tmp = str(slash_info[i]['in_slash'][k]['tokenIndex'])\n",
        "                tkidx_ans += tkidx_tmp\n",
        "                \n",
        "                \n",
        "                htkidx_tmp = str(slash_info[i]['in_slash'][k]['headTokenIndex'])\n",
        "                htkid_ans += htkidx_tmp\n",
        "                \n",
        "                if k != len(slash_info[i]['in_slash'])-1:\n",
        "                    english_ans += ' '\n",
        "                    label_ans += ' '\n",
        "                    pos_ans += ' '\n",
        "                    gram_tmp += ' '\n",
        "                    pos_label_ans += ' '\n",
        "                    tkidx_ans += ' '\n",
        "                    htkid_ans += ' '\n",
        "                    \n",
        "                else:\n",
        "                    pass\n",
        "                \n",
        "              \n",
        "            gram_tag.append(gram_tmp)\n",
        "            korean_piece.append(korean_ans)\n",
        "            english_piece.append(english_ans)\n",
        "            label_piece.append(label_ans)\n",
        "            pos_piece.append(pos_ans)\n",
        "            pos_label_piece.append(pos_label_ans)\n",
        "            tkidx_piece.append(tkidx_ans)\n",
        "            #print(\"tkidx_ans : \", tkidx_ans)\n",
        "            htkidx_piece.append(htkid_ans)\n",
        "            #print(\"htkidx_ans : \", htkid_ans)\n",
        "            english_ans = ''\n",
        "            label_ans = ''\n",
        "            pos_ans = ''\n",
        "            pos_label_ans = ''\n",
        "            tkidx_ans = ''\n",
        "            htkid_ans = ''\n",
        "            \n",
        "        #print(tkidx_piece)\n",
        "        return korean_piece, english_piece, gram_tag, label_piece, pos_piece, pos_label_piece, tkidx_piece, htkidx_piece\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eA7goZbPmPEp"
      },
      "outputs": [],
      "source": [
        "def select_all_sent_ids():\n",
        "    q_sent_id = '''select id as sent_id\n",
        "                    from sentence_tbl\n",
        "                    where sentence_tbl.structure is not null'''\n",
        "\n",
        "    cursor.execute(q_sent_id)\n",
        "    sent_id= cursor.fetchall()\n",
        "    all_sent_ids = set()\n",
        "    for i in range (0, len(sent_id)):\n",
        "        sent_ids = sent_id[i]['sent_id']\n",
        "        all_sent_ids.add(sent_ids)\n",
        "    all_sent_ids = list(all_sent_ids)\n",
        "    return all_sent_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YoRYDNXAmPEq"
      },
      "outputs": [],
      "source": [
        "def make_re_slash_final_gram(gram_tag):\n",
        "\n",
        "    slash_final_gram = []\n",
        "    for item in gram_tag:\n",
        "        #item = re.sub('[^0-9]', '', item)\n",
        "        #print(item)\n",
        "        slash_final_gram.append(item.split(' '))\n",
        "    #print(\">>>\", slash_final_gram)\n",
        "    re_slash_final_gram = []\n",
        "    for item in slash_final_gram:\n",
        "        #print(\"###\", set(item))\n",
        "        sample_list = [re.sub('[^0-9]', '', v) for v in item if v]\n",
        "        #print(set(sample_list))\n",
        "        re_slash_final_gram.append(set(sample_list))\n",
        "    \n",
        "    return re_slash_final_gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s76cp0npmPEq",
        "outputId": "2a78a39c-618b-4699-cbed-63d78dfa9c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "sentence_id = 26660045280\n",
        "word_cnt = get_sentence_word_count(sentence_id)\n",
        "slash_cnt = get_sentence_slash_count(sentence_id)\n",
        "slash_info = get_slash_all_info(word_cnt, slash_cnt, sentence_id)\n",
        "print(slash_cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aizPqQh2mPEq",
        "outputId": "3244a5c8-1562-4069-d0f7-34a868b54f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'start': '0', 'end': '5', 'translation': '\"당신이 패스트푸드를 먹을 때\"', 'in_slash': [{'word': '\"When\"', 'meanings': '[[\"~할 때\"], []]', 'gram_id': '[1020]', 'label': '\"ADVMOD\"', 'pos': '\"ADV\"', 'tokenIndex': '7', 'headTokenIndex': '9'}, {'word': '\"you\"', 'meanings': '[[], []]', 'gram_id': None, 'label': '\"NSUBJ\"', 'pos': '\"PRON\"', 'tokenIndex': '8', 'headTokenIndex': '9'}, {'word': '\"eat\"', 'meanings': '[[\"먹다\"], [\"음식\", \"식사\"]]', 'gram_id': '[201]', 'label': '\"ADVCL\"', 'pos': '\"VERB\"', 'tokenIndex': '9', 'headTokenIndex': '26'}, {'word': '\"fast\"', 'meanings': '[[\"단식하다\"], [], [\"빠른\", \" 신속한\"], [\"빨리\"]]', 'gram_id': None, 'label': '\"AMOD\"', 'pos': '\"ADJ\"', 'tokenIndex': '10', 'headTokenIndex': '11'}, {'word': '\"food\"', 'meanings': '[[\"음식\"]]', 'gram_id': None, 'label': '\"DOBJ\"', 'pos': '\"NOUN\"', 'tokenIndex': '11', 'headTokenIndex': '9'}, {'word': '\",\"', 'meanings': None, 'gram_id': None, 'label': '\"P\"', 'pos': '\"PUNCT\"', 'tokenIndex': '12', 'headTokenIndex': '11'}]}, {'start': '6', 'end': '15', 'translation': '\"피자, 햄버거, 또는 후라이드 치킨과 같은\"', 'in_slash': [{'word': '\"such\"', 'meanings': '[[], [\"이러한\", \"그러한\"]]', 'gram_id': '[3171]', 'label': '\"MWE\"', 'pos': '\"ADJ\"', 'tokenIndex': '13', 'headTokenIndex': '14'}, {'word': '\"as\"', 'meanings': '[[], [\"~만큼\", \"~로\", \"~듯이\", \"마찬가지로\"]]', 'gram_id': '[3171]', 'label': '\"PREP\"', 'pos': '\"ADP\"', 'tokenIndex': '14', 'headTokenIndex': '11'}, {'word': '\"pizza\"', 'meanings': '[[\"피자\"]]', 'gram_id': '[2065]', 'label': '\"POBJ\"', 'pos': '\"NOUN\"', 'tokenIndex': '15', 'headTokenIndex': '14'}, {'word': '\",\"', 'meanings': None, 'gram_id': '[2065]', 'label': '\"P\"', 'pos': '\"PUNCT\"', 'tokenIndex': '16', 'headTokenIndex': '15'}, {'word': '\"hamburger\"', 'meanings': '[[\"햄버거\"]]', 'gram_id': '[2065]', 'label': '\"CONJ\"', 'pos': '\"NOUN\"', 'tokenIndex': '17', 'headTokenIndex': '15'}, {'word': '\",\"', 'meanings': None, 'gram_id': '[2065]', 'label': '\"P\"', 'pos': '\"PUNCT\"', 'tokenIndex': '18', 'headTokenIndex': '15'}, {'word': '\"or\"', 'meanings': '[[]]', 'gram_id': '[2065, 1005]', 'label': '\"CC\"', 'pos': '\"CONJ\"', 'tokenIndex': '19', 'headTokenIndex': '15'}, {'word': '\"fried\"', 'meanings': '[[\"기름에 튀긴\"]]', 'gram_id': '[2065]', 'label': '\"AMOD\"', 'pos': '\"ADJ\"', 'tokenIndex': '20', 'headTokenIndex': '21'}, {'word': '\"chicken\"', 'meanings': '[[\"치킨\", \"닭\"]]', 'gram_id': '[2065]', 'label': '\"CONJ\"', 'pos': '\"NOUN\"', 'tokenIndex': '21', 'headTokenIndex': '15'}, {'word': '\",\"', 'meanings': None, 'gram_id': None, 'label': '\"P\"', 'pos': '\"PUNCT\"', 'tokenIndex': '22', 'headTokenIndex': '26'}]}, {'start': '16', 'end': '22', 'translation': '\"당신은 그것과 함께 무엇을 마시는가\"', 'in_slash': [{'word': '\"what\"', 'meanings': '[[], []]', 'gram_id': '[1304]', 'label': '\"DOBJ\"', 'pos': '\"PRON\"', 'tokenIndex': '23', 'headTokenIndex': '26'}, {'word': '\"do\"', 'meanings': '[[\"하다\", \"충분하다\", \"끝내다\"], []]', 'gram_id': '[1314, 201]', 'label': '\"AUX\"', 'pos': '\"VERB\"', 'tokenIndex': '24', 'headTokenIndex': '26'}, {'word': '\"you\"', 'meanings': '[[], []]', 'gram_id': '[1314]', 'label': '\"NSUBJ\"', 'pos': '\"PRON\"', 'tokenIndex': '25', 'headTokenIndex': '26'}, {'word': '\"drink\"', 'meanings': '[[\"음료\", \" 마실 것\"], [\"마시다\"]]', 'gram_id': '[1314]', 'label': '\"ROOT\"', 'pos': '\"VERB\"', 'tokenIndex': '26', 'headTokenIndex': '26'}, {'word': '\"with\"', 'meanings': '[[]]', 'gram_id': '[1209]', 'label': '\"PREP\"', 'pos': '\"ADP\"', 'tokenIndex': '27', 'headTokenIndex': '26'}, {'word': '\"it\"', 'meanings': '[[\"이것\"]]', 'gram_id': '[718]', 'label': '\"POBJ\"', 'pos': '\"PRON\"', 'tokenIndex': '28', 'headTokenIndex': '27'}, {'word': '\"?\"', 'meanings': '[[]]', 'gram_id': None, 'label': '\"P\"', 'pos': '\"PUNCT\"', 'tokenIndex': '29', 'headTokenIndex': '26'}]}]\n"
          ]
        }
      ],
      "source": [
        "sentence_id = 79695568212\n",
        "word_cnt = get_sentence_word_count(sentence_id)\n",
        "slash_cnt = get_sentence_slash_count(sentence_id)\n",
        "slash_info = get_slash_all_info(word_cnt, slash_cnt, sentence_id)\n",
        "print(slash_info)\n",
        "if slash_cnt == 0:\n",
        "    print('no_slash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ffxzyAcmPEr",
        "outputId": "b8f008be-79ea-461a-ad95-cd08890574bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"When\"\n",
            "\"ADV\"\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "print(slash_info[0]['in_slash'][0]['word'])\n",
        "print(slash_info[0]['in_slash'][0]['pos'])\n",
        "print(slash_info[0]['in_slash'][0]['headTokenIndex'])\n",
        "head = slash_info[0]['in_slash'][0]['headTokenIndex']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SyEylPBmPEr"
      },
      "outputs": [],
      "source": [
        "#################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWwwhWM5mPEr",
        "outputId": "c07acd42-0818-4e4f-c17b-abd64b9ffb9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>>>........ 문장 개수 :  756142\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "sent_list = select_all_sent_ids()\n",
        "print(\">>>>........ 문장 개수 : \", len(sent_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEf2O6pxmPEr",
        "outputId": "018a71e2-878c-4d88-9013-b31a9b942048"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "756142"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(sent_list))\n",
        "\n",
        "# 615013\n",
        "# 6/14 : 756142"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh0dWLIrmPEs",
        "outputId": "ace89718-d686-4956-9db1-442cf4507cb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 248917/756142 [2:08:45<4:25:37, 31.83it/s] "
          ]
        }
      ],
      "source": [
        "chunk_data = pd.DataFrame({})\n",
        "\n",
        "sent_id_add = []\n",
        "korean_piece_add = []\n",
        "english_piece_add = [] \n",
        "gram_tag_add = [] \n",
        "label_piece_add = [] \n",
        "pos_piece_add = []\n",
        "pos_label_piece_add = []\n",
        "thidx_add = []\n",
        "htkidx_add = []\n",
        "\n",
        "#sent_list = ['79695568212']\n",
        "\n",
        "for sent_id in tqdm(sent_list):\n",
        "    # print(sent_id)\n",
        "    word_cnt = get_sentence_word_count(sent_id)\n",
        "    slash_cnt = get_sentence_slash_count(sent_id)\n",
        "    slash_info = get_slash_all_info(word_cnt, slash_cnt, sent_id)\n",
        "    if slash_cnt == 0:\n",
        "        pass\n",
        "    else:\n",
        "        korean_piece, english_piece, gram_tag, label_piece, pos_piece, pos_label_piece, tkidx_piece, htkidx_piece = chunck_to_dataframe(slash_info)\n",
        "        #print(htkidx_piece)\n",
        "        final_chunck_gram = make_re_slash_final_gram(gram_tag)\n",
        "        \n",
        "        dep_str = ''\n",
        "    \n",
        "        for i in range (0, len(korean_piece)):\n",
        "            sent_id_add.append(sent_id)\n",
        "            korean_piece_add.append(korean_piece[i]) \n",
        "            english_piece_add.append(english_piece[i]) \n",
        "            gram_tag_add.append(final_chunck_gram[i])\n",
        "            label_piece_add.append(label_piece[i])\n",
        "            pos_piece_add.append(pos_piece[i])\n",
        "            pos_label_piece_add.append(pos_label_piece[i])\n",
        "            thidx_add.append(tkidx_piece[i])\n",
        "            htkidx_add.append(htkidx_piece[i])\n",
        "        \n",
        "            \n",
        "chunk_data['sent_id'] = sent_id_add\n",
        "chunk_data['english_piece'] = english_piece_add\n",
        "chunk_data['korean_piece'] = korean_piece_add\n",
        "chunk_data['gram_tag'] = gram_tag_add\n",
        "chunk_data['pos_piece'] = pos_piece_add\n",
        "chunk_data['label_piece'] = label_piece_add\n",
        "chunk_data['pos_label_piece'] = pos_label_piece_add\n",
        "chunk_data['thidx'] = thidx_add\n",
        "chunk_data['htkidx'] = htkidx_add\n",
        "\n",
        "\n",
        "chunk_data.to_csv('all_chunk_data+headidx.csv')\n",
        "# chunk_data.to_csv('head_test.csv')\n",
        "#chunk_data.to_csv('all_chunk_data+head.csv')"
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.g4dn.2xlarge",
    "kernelspec": {
      "display_name": "Python 3 (Data Science)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "1) sent_data_db_extract.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}